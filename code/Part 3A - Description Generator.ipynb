{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93483a30",
   "metadata": {},
   "source": [
    "## Defining the problem\n",
    "\n",
    "**What is Writer‚Äôs Block?**\n",
    "- Defined as ‚Äúthe inability to begin or continue writing for reasons other than a lack of basic skill or commitment.‚Äù\n",
    "- Personal Experience: Can be very frustrating, caused me sleepless nights when I was affected by it. Can also happen to artists.\n",
    "- Put plainly, it is a sudden lack of motivation which results in writers being unable to continue their work.\n",
    "\n",
    "**Storyboarding, and how writer‚Äôs block has a negative effect:**\n",
    "- Crucial step where creators plan the visual flow of the story, depicting scenes and panel layouts.\n",
    "- Storyboarding is the bridge between a written script and the final artwork.\n",
    "- Writer's block can extend to storyboarding, where creators struggle to visualize the scenes and transitions\n",
    "\n",
    "**Writer‚Äôs Block is prevalent in the manga industry:**\n",
    "- Berserk: Kentaro Miura was known for taking hiatuses due to creative challenges leading to long waits.\n",
    "- Evangelion: Yoshiyuki Sadamoto took multiple hiatuses which were attributed to creative challenges.\n",
    "- Hunter X Hunter: Yoshihiro Togashi, experienced writer's block, leading to year-long hiatuses.\n",
    "\n",
    "**Creators‚Äô relentless workload is worsening the issue:**\n",
    "- There are many manga creators who end up suffering from their work\n",
    "- Creators work long hours affecting physical or mental health or suffer direct abuse from the system.\n",
    "- Leads to a loss of motivation, inability to produce new ideas, and worsens the problem of writer‚Äôs block.\n",
    "\n",
    "**How is AI helping with writer's block?**\n",
    "- ChatGPT is helping to write next instalment of manga hit One Piece as author runs into writer‚Äôs block.\n",
    "- ‚ÄúCyberpunk: Peach John‚Äù is the world‚Äôs first complete AI manga work.\n",
    "\n",
    "We thus arrive at the following problem statement:\n",
    "\n",
    "## Problem Statement\n",
    "‚ÄúCan we build AI Models that address the problem of writer‚Äôs block in the manga industry by helping with ideation and storyboarding?‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4040e8",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Imports](#Imports)\n",
    "- [Data Preprocessing](#Data-Preprocessing)\n",
    "- [Model Tuning and Training](#Model-Tuning-And-Training)\n",
    "- [Saving the model as a tokenizer](#Saving-the-model-as-a-toeknizer)\n",
    "- [Text Generation from the Trained Model](Using-the-trained-model-to-generate-text)\n",
    "- [Evaluation](#Evaluation)\n",
    "  - [BLEU](#BLEU)\n",
    "  - [Rouge](#Rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b0dbd",
   "metadata": {},
   "source": [
    "## Dataset Used\n",
    "- filtered_manga_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee1a99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d401b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d954456",
   "metadata": {},
   "source": [
    "In this block, the necessary libraries and modules are imported. Let's break down the imports and their purposes:\n",
    "\n",
    "- pandas is imported for data manipulation, allowing us to work with tabular data in a structured way.\n",
    "- GPT2Tokenizer and GPT2LMHeadModel are imported from the Transformers library. These are key components for working with the GPT-2 model. The tokenizer is used to preprocess text data, and the LMHeadModel is the core GPT-2 language model.\n",
    "- nltk is imported, which is a natural language processing library.\n",
    "- From nltk.tokenize, the sent_tokenize function is imported, which is used for splitting text into sentences.\n",
    "- re is imported for regular expressions, which can be helpful for text cleaning and pattern matching.\n",
    "- sacrebleu and Rouge are imported to enable the calculation of BLEU and ROUGE scores, which are metrics for evaluating the quality of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a01fb",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a583662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the manga data CSV file\n",
    "df = pd.read_csv('../data/filtered_manga_data.csv')\n",
    "\n",
    "# Extract the description and genre columns\n",
    "descriptions = df['Description']\n",
    "genres = df['Primary Genre']\n",
    "\n",
    "# Combine descriptions and genres to create a list of text samples\n",
    "text_samples = [f\"A {genre} manga: {description}\" for description, genre in zip(descriptions, genres)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572a0d9",
   "metadata": {},
   "source": [
    "In this block, we start working with the manga dataset. Here's what each part of this block does:\n",
    "\n",
    "- pd.read_csv('filtered_manga_data.csv') reads the manga data from a CSV file named 'filtered_manga_data.csv' into a DataFrame named df. The DataFrame represents the tabular data with rows and columns.\n",
    "- df['Description'] and df['Primary Genre'] extract the 'Description' and 'Primary Genre' columns from the DataFrame, respectively. These columns contain the descriptions and genres of manga.\n",
    "- The text_samples list is created by combining the descriptions and genres using a list comprehension. Each entry in text_samples is a string that represents a manga description and its associated genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a239a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPT-2 tokenizer and model\n",
    "model_name = \"gpt2\"  # You can choose a specific GPT-2 variant\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Create a text file to store the text samples\n",
    "with open('text_samples.txt', 'w', encoding='utf-8') as file:\n",
    "    for sample in text_samples:\n",
    "        file.write(sample + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549b03c",
   "metadata": {},
   "source": [
    "This block initializes the GPT-2 model and tokenizer. Here's the breakdown:\n",
    "\n",
    "- model_name = \"gpt2\" specifies the GPT-2 model variant we will use. In this case, it's set to the base GPT-2 model.\n",
    "- GPT2Tokenizer.from_pretrained(model_name) initializes the tokenizer with the chosen model name.\n",
    "- GPT2LMHeadModel.from_pretrained(model_name) initializes the GPT-2 language model with the same model name.\n",
    "- A text file is created to store the text_samples using UTF-8 encoding. The content of text_samples is written to this file, with each sample separated by a newline character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71670b4",
   "metadata": {},
   "source": [
    "### Model Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd46d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1872' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1872/1872 4:01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1872, training_loss=3.116937555818476, metrics={'train_runtime': 14526.3845, 'train_samples_per_second': 1.03, 'train_steps_per_second': 0.129, 'total_flos': 976905584640000.0, 'train_loss': 3.116937555818476, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text samples and create a dataset from the file\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='text_samples.txt',  # Provide the path to the text file\n",
    "    block_size=128  # Adjust the block size as needed\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",  # Specify the output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Adjust the number of epochs\n",
    "    per_device_train_batch_size=8,  # Adjust batch size\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30948efe",
   "metadata": {},
   "source": [
    "This block tokenizes, fine tunes, and trains the model. Here is an explaination of the process:\n",
    "- train_dataset is being defined and initialized as a TextDataset. This dataset is essential for fine-tuning the GPT-2 model.- \n",
    "- tokenizer=tokenizer specifies the tokenizer we previously initialized. It will be used to tokenize the text samples.\n",
    "- file_path='text_samples.txt' specifies the path to the text file that contains our text samples. \n",
    "- block_size=128 is an important parameter. It defines the maximum sequence length for the tokenized text. Text samples that exceed this length will be split into multiple chunks. For instance, if set to 128, it means the tokenized sequences will be no longer than 128 tokens.\n",
    "\n",
    "The next part of the block sets up the data collator for language modeling:\n",
    "- data_collator is defined as a DataCollatorForLanguageModeling. It is responsible for formatting the tokenized data into batches suitable for training the model.\n",
    "- tokenizer=tokenizer links this data collator to the tokenizer used earlier.\n",
    "- mlm=False specifies that Masked Language Modeling (MLM) is not being used in this case. MLM is a technique where certain tokens are replaced with [MASK] tokens, and the model is trained to predict the original tokens. Since we're fine-tuning for text generation and not MLM, mlm is set to False.\n",
    "\n",
    "The next part of the block focuses on setting up training arguments:\n",
    "- output_dir specifies the directory where the fine-tuned model and associated files will be saved. In this case, it's set to \"./fine-tuned-model\" \n",
    "- overwrite_output_dir=True indicates that if the specified output directory already exists, it should be overwritten. Be cautious with this option, as it will replace any existing data in the output directory.\n",
    "- num_train_epochs=3 determines the number of training epochs. An epoch is a complete pass through the training data.\n",
    "- per_device_train_batch_size=8 sets the batch size for training. A batch is a set of data samples used in a single forward and backward pass during training.\n",
    "- save_steps=10_000 indicates that model checkpoints will be saved every 10,000 steps during training. A checkpoint is a saved version of the model that allows us to resume training or evaluate the model's performance.\n",
    "- save_total_limit=2 specifies the maximum number of checkpoints to keep. When this limit is reached, the oldest checkpoints will be deleted.\n",
    "- evaluation_strategy=\"steps\" defines the evaluation strategy. It means that the model will be evaluated at specified intervals defined by eval_steps.\n",
    "- eval_steps=10_000 specifies that evaluation will occur every 10,000 training steps. During evaluation, the model's performance metrics can be calculated.\n",
    "\n",
    "Finally, we set up the Trainer to start fine-tuning the model:\n",
    "- Trainer is initialized, connecting the model, training arguments, data collator, and training dataset.\n",
    "- trainer.train() starts the fine-tuning process. The model will go through the training data for the specified number of epochs, adjusting its weights and learning to generate text based on the provided samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d84a66",
   "metadata": {},
   "source": [
    "### Saving the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88e2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_fine_tuned_model\\\\tokenizer_config.json',\n",
       " './saved_fine_tuned_model\\\\special_tokens_map.json',\n",
       " './saved_fine_tuned_model\\\\vocab.json',\n",
       " './saved_fine_tuned_model\\\\merges.txt',\n",
       " './saved_fine_tuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('./saved_fine_tuned_model')\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./saved_fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57712b20",
   "metadata": {},
   "source": [
    "### Using the trained model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bdfeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Drama manga: The story of the \"Great War\" between the two countries begins with the discovery of a mysterious \"ghost\" in the clouds. The ghost is a young boy named Kyouko, who is said to be the strongest in all of Japan. He is also the son of an aristocrat and a noblewoman.\n",
      "\n",
      "Kyouka is the only child of Kyohei, a wealthy family in Japan, and his father, the head of his family, is an important\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./saved_fine_tuned_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./saved_fine_tuned_model\")\n",
    "\n",
    "# User input genre\n",
    "user_genre = \"Drama\"\n",
    "\n",
    "# Generate a single description based on the user input genre\n",
    "# Set max_length to control the length of the generated text\n",
    "input_text = f\"A {user_genre} manga:\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set attention_mask and pad_token_id\n",
    "attention_mask = input_ids.clone()\n",
    "attention_mask[attention_mask != tokenizer.pad_token_id] = 1 # Set non-pad tokens to 1\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  attention_mask=attention_mask,\n",
    "  max_length=100, # Adjust max_length as needed\n",
    "  num_return_sequences=1,\n",
    "  no_repeat_ngram_size=2,\n",
    "  pad_token_id=pad_token_id,\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734852a",
   "metadata": {},
   "source": [
    "This block focuses on generating text with the fine-tuned model and presenting it to the user. Here's a detailed breakdown:\n",
    "\n",
    "- The fine-tuned model and tokenizer are loaded from the saved directory using the from_pretrained method.\n",
    "- A user-defined genre, such as \"Romance,\" is stored in the variable user_genre.\n",
    "- We generate a description based on the user input genre. To do this, we construct an initial text using the user's genre and create input IDs for the tokenizer.\n",
    "- The model.generate() function generates text based on the input, controlling aspects like text length and repetition. The generated text is stored in the generated_text variable.\n",
    "- Finally, the generated text is decoded and printed for the user to read.\n",
    "\n",
    "It is observed the model does a good job of generating a description for the 'Romance' and 'Drama' genres, but it gets confused for the 'Action' and 'Comedy' genres. Moreover, the model generates about 3 coherent sentences for each genre, so the generated text cannot be used as is. There is some further cleaning required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9230847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Drama manga: The story of the \"Great War\" between the two countries begins with the discovery of a mysterious \"ghost\" in the clouds. The ghost is a young boy named Kyouko, who is said to be the strongest in all of Japan. He is also the son of an aristocrat and a noblewoman.\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to remove unwanted content\n",
    "pattern = r'\\(.*?\\)'  # Matches anything within parentheses\n",
    "\n",
    "# Remove unwanted patterns from the generated text\n",
    "cleaned_text = re.sub(pattern, '', generated_text)\n",
    "\n",
    "# Tokenize the cleaned text into sentences\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "# Keep only the first three sentences\n",
    "cleaned_sentences = sentences[:3]\n",
    "\n",
    "# Reconstruct the cleaned text from the first three sentences\n",
    "cleaned_text = \" \".join(cleaned_sentences)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18544d",
   "metadata": {},
   "source": [
    "In this block, a regular expression pattern is defined to remove unwanted content from the generated text. It removes text within parentheses. The cleaned text is then tokenized into sentences, and only the first three sentences are kept. The final cleaned text is reconstructed and printed.\n",
    "- A regular expression pattern, r'\\(.*?\\)', is defined. This pattern matches anything within parentheses and is used for removing unwanted content.\n",
    "- The re.sub() function is used to remove patterns that match the regular expression from the generated text, resulting in cleaned_text.\n",
    "- The sent_tokenize() function is used to split the cleaned text into sentences, which are stored in the sentences list.\n",
    "- To keep the text concise, only the first three sentences are retained in the cleaned_sentences list.\n",
    "- The final cleaned_text is reconstructed by joining these sentences with spaces and is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5a959",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "For text generation models like GPT-2, numerical metrics like BLEU, ROUGE, and others can provide some automated assessment of the generated text. However, they have limitations and may not always capture the true quality or relevance of the generated content accurately. These metrics are based on n-grams, word overlaps, and other statistical measures and may not fully account for coherence, contextuality, or meaningfulness.\n",
    "\n",
    "Subjective human evaluation, where human judges read and interpret the generated text, can provide a more comprehensive assessment of text quality, relevance, and fluency. Humans can better judge aspects like naturalness, context appropriateness, and overall coherence. This kind of evaluation is often considered the gold standard for assessing the quality of generated text.\n",
    "\n",
    "That being said, we still need to conduct some basic evaluations for the model. These should not be taken too seriously, as ultimately, text coherence is the best indicator of model performance.\n",
    "\n",
    "To evaluate the model, a reference.txt file was created. This file contains 4 new descriptions for the model on the trained genres. To get a more comprehensive evaluation, the size of the reference data can be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc30518",
   "metadata": {},
   "source": [
    "### Defining the Metrics\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** ROUGE is a set of metrics for the automatic evaluation of machine-generated text, primarily used in the field of natural language processing. It measures the similarity between the generated text and reference text(s). ROUGE includes various metrics like ROUGE-N (measuring n-gram overlap), ROUGE-L (measuring the longest common subsequence), ROUGE-W (measuring weighted word overlap), and more. ROUGE is often used in tasks such as machine translation, text summarization, and text generation to assess the quality of the generated text concerning reference text(s).\n",
    "- **BLEU (Bilingual Evaluation Understudy):** BLEU is another metric used to evaluate the quality of machine-generated text, particularly in the context of machine translation. BLEU measures the overlap in n-grams (contiguous sequences of n words) between the generated text and reference text(s). It is designed to provide a simple and automated way to assess the quality of machine translation output. BLEU scores are typically computed for multiple reference translations to capture variability in human-generated translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345086a4",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efbb971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores: BLEU = 0.17 1.3/0.2/0.1/0.0 (BP = 1.000 ratio = 50.167 hyp_len = 301 ref_len = 6)\n"
     ]
    }
   ],
   "source": [
    "# Load the reference texts\n",
    "reference_texts = []\n",
    "# Read the text samples from the file with 'utf-8' encoding\n",
    "with open(\"reference.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        reference_texts.append(line.strip())\n",
    "\n",
    "# Generate texts\n",
    "generated_texts = []\n",
    "for genre in [\"Romance\", \"Action\", \"Drama\", \"Comedy\"]:\n",
    "    input_text = f\"A {genre} manga:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = input_ids.clone()\n",
    "    attention_mask[attention_mask != tokenizer.pad_token_id] = 1\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=pad_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "\n",
    "# Evaluate metrics\n",
    "bleu_scores = corpus_bleu(reference_texts, generated_texts)  # Import the corpus_bleu function from sacrebleu\n",
    "\n",
    "print(\"BLEU scores:\", bleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23e399",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "The BLEU (Bilingual Evaluation Understudy) score is shown in the following format:\n",
    "\n",
    "BLEU = 0.17: This is the overall BLEU score, which is a value between 0 and 1. In this case, the BLEU score is 0.17, indicating the quality of the generated text. A higher BLEU score generally suggests better quality and relevance of the generated text compared to reference data.\n",
    "\n",
    "1.3/0.2/0.1/0.0: These are the individual n-gram precision scores, where each number corresponds to a different n-gram size (unigram, bigram, trigram, and so on). They represent how well the generated text matches the reference data for different n-gram sizes. In this case, the individual n-gram scores are 1.3 for unigrams, 0.2 for bigrams, 0.1 for trigrams, and 0.0 for higher-order n-grams. These scores indicate how well the generated text matches the reference text for each n-gram size.\n",
    "\n",
    "(BP = 1.000 ratio = 50.167 hyp_len = 301 ref_len = 6): These are additional statistics related to BLEU scoring:\n",
    "\n",
    "BP (Brevity Penalty) = 1.000: The Brevity Penalty is a value that penalizes generated text that is shorter than the reference text. A BP value of 1.000 suggests that the generated text length is similar to the reference text length.\n",
    "\n",
    "Ratio = 50.167: The ratio of the length of the generated text (hyp_len) to the length of the reference text (ref_len). In this case, the generated text is approximately 50.167 times longer than the reference text.\n",
    "\n",
    "hyp_len = 301: The length of the generated text in terms of tokens or words.\n",
    "\n",
    "ref_len = 6: The length of the reference text in terms of tokens or words.\n",
    "\n",
    "In summary, a BLEU score of 0.17 with individual n-gram scores ranging from 0.0 to 1.3 suggests that the generated text matches the reference text to some extent but may not be a very close match, especially for higher-order n-grams. The Brevity Penalty (BP) of 1.000 indicates that the length of the generated text is not significantly different from the reference text length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc7e58",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbaffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores:\n",
      "Pair 1: {'rougeL': Score(precision=0.25925925925925924, recall=0.1728395061728395, fmeasure=0.2074074074074074)}\n",
      "Pair 2: {'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
      "Pair 3: {'rougeL': Score(precision=0.15584415584415584, recall=0.15584415584415584, fmeasure=0.15584415584415584)}\n",
      "Pair 4: {'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROUGE scores for each pair\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_scores = []\n",
    "for gen_text, ref_text in zip(generated_texts, reference_texts):\n",
    "    scores = scorer.score(gen_text, ref_text)\n",
    "    rouge_scores.append(scores)\n",
    "\n",
    "print(\"ROUGE scores:\")\n",
    "for idx, scores in enumerate(rouge_scores):\n",
    "    print(f\"Pair {idx+1}: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67caac56",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "RougeL scores measure the F1 score of the precision and recall of a generated text when compared to a reference text. The closer the F1 score is to 1, the better the generated text matches the reference text.\n",
    "\n",
    "In our results, the ROUGEL scores for the first and third pairs are relatively high, indicating that the generated texts match the reference texts well. The F1 scores for these pairs are 0.207 and 0.155, respectively.\n",
    "\n",
    "However, the ROUGEL score for the second and fourth pair are very low, with an F1 score of 0. The precision and recall for these pairs are both 0, indicating that the generated text does not match the reference text at all. This suggests that the model may have had difficulty generating a coherent text for these particular genres.\n",
    "\n",
    "Overall, the ROUGE scores suggest that the model is able to generate texts that are somewhat similar to the reference texts, but there is still room for improvement. The model may need to be trained on more data or be fine-tuned to better match the style of the reference texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847363c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this part, we trained a GPT 2 model to generate manga descriptions based on user inputted genre, thus automating scenario and idea generation. The model can be improved by tuning or using a different base (such as Facebook BART), but due to time constraints, these methods are not explored here. This model works as a proof of concept, and generates 3 coherent sentences of manga descriptions.\n",
    "\n",
    "This marks the end of this portion. Now we will try and build an image generator to help automate the storyboarding process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
